# ============================================================================
# Qwen3-VL-235B-A22B-Thinking-FP8 vLLM Service Configuration
# Environment variables for the vLLM 0.11.0 service
# ============================================================================
#
# This is the FP8 quantized version of Qwen3-VL-235B-A22B-Thinking
# FP8 quantization provides ~50% memory reduction with minimal performance loss
#
# ============================================================================

# ----------------------------------------------------------------------------
# SYSTEM COMPATIBILITY
# ----------------------------------------------------------------------------

# VLLM_USE_V1: Controls vLLM architecture version
#   Value: 1 (V1 is the only architecture in vLLM 0.11.0+)
#   Current: 1 (V1 engine - legacy V0 engine removed in 0.11.0)
#   
#   Note: vLLM 0.11.0+ only has V1 engine with native Qwen3-VL support.
#   This engine is optimized for MoE models and multi-GPU setups.
export VLLM_USE_V1=1

# PYTORCH_CUDA_ALLOC_CONF: PyTorch CUDA memory allocator configuration
#   Value: expandable_segments:True
#   Purpose: Reduces memory fragmentation during model initialization
#   Impact: Helps prevent OOM errors during large model loading
#   
#   How it works:
#     ✓ Allows PyTorch to expand memory segments dynamically
#     ✓ Reduces wasted space from fragmentation
#     ✓ Particularly helpful for MoE models with 128 experts
#   
#   This is critical for the 236B model's initialization phase
export PYTORCH_ALLOC_CONF=expandable_segments:True

# CUDA_HOME: Path to CUDA installation directory
#   Purpose: Tells the system where CUDA libraries are installed
#   Change: Update if your CUDA is installed in a different location
export CUDA_HOME=/usr/local/cuda-12.9
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

# ----------------------------------------------------------------------------
# MODEL CONFIGURATION
# ----------------------------------------------------------------------------

# MODEL_PATH: Path to the FP8 quantized model weights and configuration files
#   Purpose: Location of the Qwen3-VL-235B-A22B-Thinking-FP8 model on disk
#   Change: Update if you move the model to a different directory
MODEL_PATH="/home/naresh/models/qwen3-vl-235b-thinking-fp8"

# PORT: Network port where the API server listens
#   Default: 8010
#   Impact: Client requests must use this port (http://localhost:8010)
#   Change: Use a different port if 8010 is already in use
PORT=8010

# HOST: Network interface to bind the server
#   "0.0.0.0": Listen on all network interfaces (accessible from any IP)
#   "127.0.0.1": Listen only on localhost (more secure, local access only)
#   Impact: Controls whether remote machines can access the service
HOST="0.0.0.0"

# DTYPE: Data type precision for model weights
#   Options: "float16", "bfloat16", "float32"
#   "float16": Recommended for FP8 models (RECOMMENDED for FP8)
#   "bfloat16": Alternative precision
#   "float32": Highest precision, but uses more memory
#   Impact: FP8 weights are loaded and computed in float16/bfloat16
DTYPE="float16"

# ----------------------------------------------------------------------------
# PERFORMANCE & CAPACITY SETTINGS
# ----------------------------------------------------------------------------

# MAX_MODEL_LEN: Maximum context length (input + output tokens)
#   Purpose: Controls how much input the model can process at once
#   Current: 32768 tokens (conservative start)
#   Maximum: 262144 tokens (native 256K support), expandable to 1M
#   
#   Impact of increasing:
#     ✓ Can process longer videos/documents
#     ✗ Uses MORE GPU memory per request
#     ✗ SLOWER inference (more computation)
#     ✗ Reduces MAX_NUM_SEQS capacity
#   
#   Recommended values:
#     32768  = Standard tasks, 4-8 concurrent requests  (CURRENT - SAFE START)
#     65536  = Long videos/documents, 2-4 concurrent requests
#     131072 = Very long content, 1-2 concurrent requests
#     262144 = Full native context, single request at a time
MAX_MODEL_LEN=32768

# TENSOR_PARALLEL_SIZE: Number of GPUs to split the model across
#   Purpose: Distributes model weights across multiple GPUs
#   Current: 8 (using all 8 H100 GPUs) - RECOMMENDED for stability
#   
#   Impact:
#     ✓ FP8 model is ~236GB (50% smaller than BF16's ~472GB)
#     ✓ Could work with 4 GPUs but 8 provides better throughput
#     ✓ Provides 640GB total VRAM (8 × 80GB)
#     ✗ Slight communication overhead between GPUs
#   
#   Rule: Must match the number of available GPUs in SLURM allocation
#   Note: 8 GPUs recommended for flexibility and concurrent requests
TENSOR_PARALLEL_SIZE=8

# GPU_MEMORY_UTIL: Fraction of GPU memory to use for model
#   Range: 0.0 to 1.0 (0.90 = 90% of available memory)
#   Current: 0.90 (uses ~72GB per H100 out of 80GB) - CONSERVATIVE START
#   
#   Impact of increasing (e.g., 0.95):
#     ✓ Can handle slightly more concurrent requests
#     ✗ Higher risk of Out-of-Memory (OOM) errors
#     ✗ Less room for memory spikes
#   
#   Impact of decreasing (e.g., 0.85):
#     ✓ More stable, less likely to crash
#     ✓ Leaves room for other processes
#     ✗ Fewer concurrent requests possible
#   
#   Recommended: Start at 0.90, tune to 0.95 after testing
GPU_MEMORY_UTIL=0.90

# MAX_NUM_SEQS: Maximum number of concurrent sequences (batch size)
#   Purpose: How many requests can be processed simultaneously
#   Current: 4 concurrent requests (conservative for 236B model)
#   
#   Impact of increasing (e.g., 8):
#     ✓ Higher throughput (more requests handled)
#     ✗ Requires MORE GPU memory
#     ✗ May cause OOM if MAX_MODEL_LEN is large
#   
#   Impact of decreasing (e.g., 2):
#     ✓ Uses LESS memory, more stable
#     ✓ Better for long videos (high MAX_MODEL_LEN)
#     ✗ Lower throughput
#   
#   Relationship with MAX_MODEL_LEN:
#     MAX_MODEL_LEN=32768  → MAX_NUM_SEQS=4-8   (current default)
#     MAX_MODEL_LEN=65536  → MAX_NUM_SEQS=2-4   (reduce concurrency)
#     MAX_MODEL_LEN=131072 → MAX_NUM_SEQS=1-2   (further reduce)
MAX_NUM_SEQS=2

# --------------------------------------------------MAX_NUM_SEQS--------------------------
# MULTIMODAL INPUT LIMITS
# ----------------------------------------------------------------------------

# LIMIT_MM_PER_PROMPT: Limits for multimodal inputs per request
#   Purpose: Prevents memory exhaustion from too many images/videos
#   Format: JSON dictionary - '{"image": N, "video": M}' (vLLM 0.10+ requires JSON format)
#   Current: '{"image": 4, "video": 1}' (4 images OR 1 video per request)
#   
#   Impact:
#     ✓ Prevents OOM from excessive multimodal inputs
#     ✓ Ensures stable service operation
#     ✗ Limits complex multi-image/video scenarios
#   
#   Tuning:
#     - Increase image limit for multi-image tasks (e.g., '{"image": 8, "video": 1}')
#     - Video processing is memory-intensive, keep video=1
#   
#   IMPORTANT: Must be valid JSON format for vLLM 0.10+
LIMIT_MM_PER_PROMPT='{"image": 4, "video": 1}'

# ----------------------------------------------------------------------------
# GENERATION PARAMETERS (How the model generates responses)
# ----------------------------------------------------------------------------

# TEMPERATURE: Controls randomness in token selection
#   Range: 0.0 to 2.0
#   Current: 0.7 (balanced creativity and coherence)
#   
#   Lower (0.1-0.3):
#     ✓ More deterministic, focused, consistent
#     ✓ Better for factual tasks (OCR, transcription)
#     ✗ Less creative, may repeat phrases
#   
#   Medium (0.5-0.8):
#     ✓ Balanced creativity and coherence (RECOMMENDED)
#     ✓ Good for image/video analysis, descriptions
#   
#   Higher (0.9-1.5):
#     ✓ More creative, diverse outputs
#     ✗ Less predictable, may lose focus
#     ✗ Can generate nonsensical text
#   
#   Impact: Higher = more random/creative, Lower = more focused/deterministic
TEMPERATURE=0.7

# TOP_P: Nucleus sampling - cumulative probability threshold
#   Range: 0.0 to 1.0
#   Current: 0.95 (consider top 95% probability mass)
#   
#   How it works:
#     Model considers tokens until cumulative probability reaches TOP_P
#     Filters out unlikely tokens dynamically
#   
#   Lower (0.5-0.7):
#     ✓ More conservative, safer word choices
#     ✓ Reduces hallucination
#     ✗ Less diverse vocabulary
#   
#   Medium (0.8-0.95):
#     ✓ Good balance (RECOMMENDED)
#     ✓ Natural language flow
#   
#   Higher (0.95-1.0):
#     ✓ Maximum diversity
#     ✗ May include unlikely words
#   
#   Typical use: Works together with TEMPERATURE
#   Recommended: 0.90-0.95 for most tasks
TOP_P=0.95

# TOP_K: Limits vocabulary to K most likely tokens
#   Range: 1 to vocab_size (typically 10-100)
#   Current: 20 (consider only top 20 most likely tokens)
#   
#   How it works:
#     At each step, only consider the K most probable next tokens
#     Filters out very unlikely tokens
#   
#   Lower (5-15):
#     ✓ More focused, deterministic
#     ✓ Reduces nonsensical outputs
#     ✗ Limited vocabulary, less natural
#   
#   Medium (20-40):
#     ✓ Good balance (RECOMMENDED)
#     ✓ Natural while preventing wild choices
#   
#   Higher (50-100):
#     ✓ More diverse vocabulary
#     ✗ May generate less coherent text
#   
#   Note: Works together with TOP_P - whichever is more restrictive applies
#   Typical values: 20-50 for general use
TOP_K=20

# MAX_TOKENS: Maximum number of tokens to generate in response
#   Range: 1 to MAX_MODEL_LEN
#   Current: 8192 (~6,000 words)
#   
#   How it works:
#     Sets upper limit on response length
#     Model stops generating when it reaches this limit OR completes naturally
#   
#   Impact:
#     ✓ Prevents overly long responses
#     ✓ Controls API costs/latency
#     ✗ May truncate detailed analyses if too low
#   
#   Considerations:
#     Total tokens used = Input tokens + MAX_TOKENS
#     Must fit within MAX_MODEL_LEN
#   
#   Recommended values:
#     4096  = Short answers (~3,000 words)
#     8192  = Medium responses (~6,000 words) (CURRENT)
#     16384 = Long detailed analyses (~12,000 words)
#     32768 = Very comprehensive reports (requires high MAX_MODEL_LEN)
MAX_TOKENS=8192

# ----------------------------------------------------------------------------
# PARAMETER TUNING GUIDE
# ----------------------------------------------------------------------------
#
# For FACTUAL/PRECISE tasks (OCR, text extraction, object detection):
#   TEMPERATURE=0.2, TOP_P=0.9, TOP_K=10
#
# For BALANCED tasks (image/video analysis, general Q&A):
#   TEMPERATURE=0.7, TOP_P=0.95, TOP_K=20  (CURRENT DEFAULT)
#
# For CREATIVE tasks (storytelling, artistic descriptions):
#   TEMPERATURE=1.0, TOP_P=0.95, TOP_K=50
#
# For VERY SAFE/CONSERVATIVE output:
#   TEMPERATURE=0.1, TOP_P=0.85, TOP_K=10
#
# ============================================================================
# MODEL-SPECIFIC NOTES - FP8 VERSION
# ============================================================================
#
# Qwen3-VL-235B-A22B-Thinking-FP8 is an FP8 quantized version:
# - Total parameters: 236B (same as BF16)
# - Active parameters per forward pass: ~22B (128 experts, 8 active)
# - Quantization: Fine-grained FP8 with block size 128
# - Performance: Nearly identical to BF16 version
# - Memory advantage: ~50% reduction (236GB vs 472GB)
# - Architecture: Mixture-of-Experts (MoE)
# 
# Key capabilities:
#   * Visual agent interactions (GUI understanding)
#   * Visual coding (HTML/CSS/JS generation from images)
#   * Advanced spatial perception and 3D grounding
#   * Long context: 256K native, expandable to 1M tokens
#   * Enhanced OCR in 32 languages
#   * Video understanding with temporal modeling
#
# Memory requirements (FP8):
# - Model weights: ~236GB in FP8
# - Minimum GPUs: 4x H100 80GB (320GB total) - tight fit
# - Recommended: 8x H100 80GB (640GB total) - comfortable
# - Recommended allocation: See SLURM command in README.md
#
# ============================================================================
# vLLM 0.11.0 SPECIFIC NOTES
# ============================================================================
#
# - V1 engine is the ONLY engine in 0.11.0 (V0 completely removed)
# - Auto-detection of V1 engine - no manual configuration needed
# - Native Qwen3-VL support added in this version
# - Requires qwen-vl-utils==0.0.14 for vision processing
# - FP8 quantization supported natively
# - Expected performance: 3-5x faster than transformers
#
# ============================================================================

