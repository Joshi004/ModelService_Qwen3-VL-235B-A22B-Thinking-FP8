INFO 12-02 09:53:46 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:53:46 [api_server.py:1977] vLLM API server version 0.11.2
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:53:46 [utils.py:253] non-default args: {'model_tag': '/home/naresh/models/qwen3-vl-235b-thinking-fp8', 'host': '0.0.0.0', 'port': 8010, 'model': '/home/naresh/models/qwen3-vl-235b-thinking-fp8', 'trust_remote_code': True, 'max_model_len': 131072, 'tensor_parallel_size': 8, 'enable_expert_parallel': True, 'max_num_seqs': 2}
[1;36m(APIServer pid=3057302)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:53:46 [model.py:631] Resolved architecture: Qwen3VLMoeForConditionalGeneration
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:53:46 [model.py:1745] Using max model len 131072
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:53:47 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3057393)[0;0m INFO 12-02 09:53:57 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='/home/naresh/models/qwen3-vl-235b-thinking-fp8', speculative_config=None, tokenizer='/home/naresh/models/qwen3-vl-235b-thinking-fp8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/naresh/models/qwen3-vl-235b-thinking-fp8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none', '+quant_fp8'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 4, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3057393)[0;0m WARNING 12-02 09:53:57 [multiproc_executor.py:869] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 12-02 09:54:12 [parallel_state.py:1208] world_size=8 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:40263 backend=nccl
INFO 12-02 09:54:12 [parallel_state.py:1208] world_size=8 rank=6 local_rank=6 distributed_init_method=tcp://127.0.0.1:40263 backend=nccl
INFO 12-02 09:54:13 [parallel_state.py:1208] world_size=8 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:40263 backend=nccl
INFO 12-02 09:54:13 [parallel_state.py:1208] world_size=8 rank=7 local_rank=7 distributed_init_method=tcp://127.0.0.1:40263 backend=nccl
INFO 12-02 09:54:13 [parallel_state.py:1208] world_size=8 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:40263 backend=nccl
INFO 12-02 09:54:13 [parallel_state.py:1208] world_size=8 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:40263 backend=nccl
INFO 12-02 09:54:13 [parallel_state.py:1208] world_size=8 rank=5 local_rank=5 distributed_init_method=tcp://127.0.0.1:40263 backend=nccl
INFO 12-02 09:54:13 [parallel_state.py:1208] world_size=8 rank=4 local_rank=4 distributed_init_method=tcp://127.0.0.1:40263 backend=nccl
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
INFO 12-02 09:54:13 [pynccl.py:111] vLLM is using nccl==2.27.5
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
INFO 12-02 09:54:16 [parallel_state.py:1394] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6
INFO 12-02 09:54:16 [parallel_state.py:1394] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7
INFO 12-02 09:54:16 [parallel_state.py:1394] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5
INFO 12-02 09:54:16 [parallel_state.py:1394] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 12-02 09:54:16 [parallel_state.py:1394] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-02 09:54:16 [parallel_state.py:1394] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-02 09:54:16 [parallel_state.py:1394] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 12-02 09:54:16 [parallel_state.py:1394] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m INFO 12-02 09:54:21 [gpu_model_runner.py:3259] Starting to load model /home/naresh/models/qwen3-vl-235b-thinking-fp8...
[1;36m(Worker_TP3_EP3 pid=3057464)[0;0m INFO 12-02 09:54:22 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(Worker_TP3_EP3 pid=3057464)[0;0m INFO 12-02 09:54:22 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(Worker_TP3_EP3 pid=3057464)[0;0m INFO 12-02 09:54:22 [layer.py:342] Enabled separate cuda stream for MoE shared_experts
[1;36m(Worker_TP3_EP3 pid=3057464)[0;0m INFO 12-02 09:54:22 [layer.py:468] [EP Rank 3/8] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/128. Experts local to global index map: 0->48, 1->49, 2->50, 3->51, 4->52, 5->53, 6->54, 7->55, 8->56, 9->57, 10->58, 11->59, 12->60, 13->61, 14->62, 15->63.
[1;36m(Worker_TP3_EP3 pid=3057464)[0;0m WARNING 12-02 09:54:22 [fp8.py:165] DeepGEMM backend requested but not available.
[1;36m(Worker_TP3_EP3 pid=3057464)[0;0m INFO 12-02 09:54:22 [fp8.py:180] Using Triton backend for FP8 MoE
[1;36m(Worker_TP4_EP4 pid=3057465)[0;0m INFO 12-02 09:54:22 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(Worker_TP4_EP4 pid=3057465)[0;0m INFO 12-02 09:54:22 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(Worker_TP4_EP4 pid=3057465)[0;0m INFO 12-02 09:54:22 [layer.py:342] Enabled separate cuda stream for MoE shared_experts
[1;36m(Worker_TP6_EP6 pid=3057467)[0;0m INFO 12-02 09:54:22 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(Worker_TP6_EP6 pid=3057467)[0;0m INFO 12-02 09:54:22 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(Worker_TP1_EP1 pid=3057462)[0;0m INFO 12-02 09:54:22 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(Worker_TP1_EP1 pid=3057462)[0;0m INFO 12-02 09:54:22 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(Worker_TP6_EP6 pid=3057467)[0;0m INFO 12-02 09:54:22 [layer.py:342] Enabled separate cuda stream for MoE shared_experts
[1;36m(Worker_TP1_EP1 pid=3057462)[0;0m INFO 12-02 09:54:22 [layer.py:342] Enabled separate cuda stream for MoE shared_experts
[1;36m(Worker_TP4_EP4 pid=3057465)[0;0m INFO 12-02 09:54:22 [layer.py:468] [EP Rank 4/8] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/128. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71, 8->72, 9->73, 10->74, 11->75, 12->76, 13->77, 14->78, 15->79.
[1;36m(Worker_TP4_EP4 pid=3057465)[0;0m WARNING 12-02 09:54:22 [fp8.py:165] DeepGEMM backend requested but not available.
[1;36m(Worker_TP4_EP4 pid=3057465)[0;0m INFO 12-02 09:54:22 [fp8.py:180] Using Triton backend for FP8 MoE
[1;36m(Worker_TP6_EP6 pid=3057467)[0;0m INFO 12-02 09:54:22 [layer.py:468] [EP Rank 6/8] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/128. Experts local to global index map: 0->96, 1->97, 2->98, 3->99, 4->100, 5->101, 6->102, 7->103, 8->104, 9->105, 10->106, 11->107, 12->108, 13->109, 14->110, 15->111.
[1;36m(Worker_TP6_EP6 pid=3057467)[0;0m WARNING 12-02 09:54:22 [fp8.py:165] DeepGEMM backend requested but not available.
[1;36m(Worker_TP6_EP6 pid=3057467)[0;0m INFO 12-02 09:54:22 [fp8.py:180] Using Triton backend for FP8 MoE
[1;36m(Worker_TP1_EP1 pid=3057462)[0;0m INFO 12-02 09:54:22 [layer.py:468] [EP Rank 1/8] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/128. Experts local to global index map: 0->16, 1->17, 2->18, 3->19, 4->20, 5->21, 6->22, 7->23, 8->24, 9->25, 10->26, 11->27, 12->28, 13->29, 14->30, 15->31.
[1;36m(Worker_TP1_EP1 pid=3057462)[0;0m WARNING 12-02 09:54:22 [fp8.py:165] DeepGEMM backend requested but not available.
[1;36m(Worker_TP1_EP1 pid=3057462)[0;0m INFO 12-02 09:54:22 [fp8.py:180] Using Triton backend for FP8 MoE
[1;36m(Worker_TP5_EP5 pid=3057466)[0;0m INFO 12-02 09:54:22 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(Worker_TP5_EP5 pid=3057466)[0;0m INFO 12-02 09:54:22 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(Worker_TP2_EP2 pid=3057463)[0;0m INFO 12-02 09:54:22 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(Worker_TP2_EP2 pid=3057463)[0;0m INFO 12-02 09:54:22 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(Worker_TP5_EP5 pid=3057466)[0;0m INFO 12-02 09:54:22 [layer.py:342] Enabled separate cuda stream for MoE shared_experts
[1;36m(Worker_TP2_EP2 pid=3057463)[0;0m INFO 12-02 09:54:22 [layer.py:342] Enabled separate cuda stream for MoE shared_experts
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m INFO 12-02 09:54:22 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m INFO 12-02 09:54:22 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m INFO 12-02 09:54:22 [layer.py:342] Enabled separate cuda stream for MoE shared_experts
[1;36m(Worker_TP2_EP2 pid=3057463)[0;0m INFO 12-02 09:54:22 [layer.py:468] [EP Rank 2/8] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/128. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39, 8->40, 9->41, 10->42, 11->43, 12->44, 13->45, 14->46, 15->47.
[1;36m(Worker_TP2_EP2 pid=3057463)[0;0m WARNING 12-02 09:54:22 [fp8.py:165] DeepGEMM backend requested but not available.
[1;36m(Worker_TP2_EP2 pid=3057463)[0;0m INFO 12-02 09:54:22 [fp8.py:180] Using Triton backend for FP8 MoE
[1;36m(Worker_TP5_EP5 pid=3057466)[0;0m INFO 12-02 09:54:22 [layer.py:468] [EP Rank 5/8] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/128. Experts local to global index map: 0->80, 1->81, 2->82, 3->83, 4->84, 5->85, 6->86, 7->87, 8->88, 9->89, 10->90, 11->91, 12->92, 13->93, 14->94, 15->95.
[1;36m(Worker_TP5_EP5 pid=3057466)[0;0m WARNING 12-02 09:54:22 [fp8.py:165] DeepGEMM backend requested but not available.
[1;36m(Worker_TP5_EP5 pid=3057466)[0;0m INFO 12-02 09:54:22 [fp8.py:180] Using Triton backend for FP8 MoE
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m INFO 12-02 09:54:22 [layer.py:468] [EP Rank 0/8] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/128. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7, 8->8, 9->9, 10->10, 11->11, 12->12, 13->13, 14->14, 15->15.
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m WARNING 12-02 09:54:22 [fp8.py:165] DeepGEMM backend requested but not available.
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m INFO 12-02 09:54:22 [fp8.py:180] Using Triton backend for FP8 MoE
[1;36m(Worker_TP7_EP7 pid=3057468)[0;0m INFO 12-02 09:54:22 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(Worker_TP7_EP7 pid=3057468)[0;0m INFO 12-02 09:54:22 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(Worker_TP7_EP7 pid=3057468)[0;0m INFO 12-02 09:54:22 [layer.py:342] Enabled separate cuda stream for MoE shared_experts
[1;36m(Worker_TP7_EP7 pid=3057468)[0;0m INFO 12-02 09:54:22 [layer.py:468] [EP Rank 7/8] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 16/128. Experts local to global index map: 0->112, 1->113, 2->114, 3->115, 4->116, 5->117, 6->118, 7->119, 8->120, 9->121, 10->122, 11->123, 12->124, 13->125, 14->126, 15->127.
[1;36m(Worker_TP7_EP7 pid=3057468)[0;0m WARNING 12-02 09:54:22 [fp8.py:165] DeepGEMM backend requested but not available.
[1;36m(Worker_TP7_EP7 pid=3057468)[0;0m INFO 12-02 09:54:22 [fp8.py:180] Using Triton backend for FP8 MoE
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/24 [00:00<?, ?it/s]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:   4% Completed | 1/24 [00:03<01:30,  3.95s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:   8% Completed | 2/24 [00:09<01:42,  4.67s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  12% Completed | 3/24 [00:14<01:43,  4.92s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  17% Completed | 4/24 [00:19<01:40,  5.02s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  21% Completed | 5/24 [00:24<01:36,  5.07s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  25% Completed | 6/24 [00:29<01:31,  5.10s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  29% Completed | 7/24 [00:34<01:26,  5.11s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  33% Completed | 8/24 [00:40<01:21,  5.09s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  38% Completed | 9/24 [00:45<01:16,  5.07s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  42% Completed | 10/24 [00:50<01:10,  5.06s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  46% Completed | 11/24 [00:55<01:05,  5.05s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  50% Completed | 12/24 [01:00<01:00,  5.06s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  54% Completed | 13/24 [01:05<00:55,  5.05s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  58% Completed | 14/24 [01:10<00:50,  5.05s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  62% Completed | 15/24 [01:15<00:45,  5.07s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  67% Completed | 16/24 [01:20<00:40,  5.08s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  71% Completed | 17/24 [01:25<00:35,  5.07s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  75% Completed | 18/24 [01:30<00:30,  5.06s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  79% Completed | 19/24 [01:35<00:25,  5.05s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  83% Completed | 20/24 [01:40<00:20,  5.05s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  88% Completed | 21/24 [01:45<00:15,  5.04s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  92% Completed | 22/24 [01:50<00:10,  5.05s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards:  96% Completed | 23/24 [01:55<00:05,  5.04s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards: 100% Completed | 24/24 [01:59<00:00,  4.66s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Loading safetensors checkpoint shards: 100% Completed | 24/24 [01:59<00:00,  4.98s/it]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m 
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m INFO 12-02 09:56:22 [default_loader.py:314] Loading weights took 119.55 seconds
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m INFO 12-02 09:56:23 [gpu_model_runner.py:3338] Model loading took 28.2359 GiB memory and 120.195202 seconds
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m INFO 12-02 09:56:23 [gpu_model_runner.py:4088] Encoder cache will be initialized with a budget of 153600 tokens, and profiled with 1 video items of the maximum feature size.
[1;36m(Worker_TP3_EP3 pid=3057464)[0;0m INFO 12-02 09:56:23 [gpu_model_runner.py:4088] Encoder cache will be initialized with a budget of 153600 tokens, and profiled with 1 video items of the maximum feature size.
[1;36m(Worker_TP5_EP5 pid=3057466)[0;0m INFO 12-02 09:56:23 [gpu_model_runner.py:4088] Encoder cache will be initialized with a budget of 153600 tokens, and profiled with 1 video items of the maximum feature size.
[1;36m(Worker_TP4_EP4 pid=3057465)[0;0m INFO 12-02 09:56:23 [gpu_model_runner.py:4088] Encoder cache will be initialized with a budget of 153600 tokens, and profiled with 1 video items of the maximum feature size.
[1;36m(Worker_TP2_EP2 pid=3057463)[0;0m INFO 12-02 09:56:23 [gpu_model_runner.py:4088] Encoder cache will be initialized with a budget of 153600 tokens, and profiled with 1 video items of the maximum feature size.
[1;36m(Worker_TP6_EP6 pid=3057467)[0;0m INFO 12-02 09:56:23 [gpu_model_runner.py:4088] Encoder cache will be initialized with a budget of 153600 tokens, and profiled with 1 video items of the maximum feature size.
[1;36m(Worker_TP7_EP7 pid=3057468)[0;0m INFO 12-02 09:56:23 [gpu_model_runner.py:4088] Encoder cache will be initialized with a budget of 153600 tokens, and profiled with 1 video items of the maximum feature size.
[1;36m(Worker_TP1_EP1 pid=3057462)[0;0m INFO 12-02 09:56:23 [gpu_model_runner.py:4088] Encoder cache will be initialized with a budget of 153600 tokens, and profiled with 1 video items of the maximum feature size.
[1;36m(Worker_TP1_EP1 pid=3057462)[0;0m /home/naresh/venvs/qwen3-vl-fp8-service/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1692: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1;36m(Worker_TP1_EP1 pid=3057462)[0;0m   torch._dynamo.utils.warn_once(msg)
[1;36m(Worker_TP7_EP7 pid=3057468)[0;0m /home/naresh/venvs/qwen3-vl-fp8-service/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1692: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1;36m(Worker_TP7_EP7 pid=3057468)[0;0m   torch._dynamo.utils.warn_once(msg)
[1;36m(Worker_TP6_EP6 pid=3057467)[0;0m /home/naresh/venvs/qwen3-vl-fp8-service/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1692: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1;36m(Worker_TP6_EP6 pid=3057467)[0;0m   torch._dynamo.utils.warn_once(msg)
[1;36m(Worker_TP5_EP5 pid=3057466)[0;0m /home/naresh/venvs/qwen3-vl-fp8-service/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1692: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1;36m(Worker_TP5_EP5 pid=3057466)[0;0m   torch._dynamo.utils.warn_once(msg)
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m /home/naresh/venvs/qwen3-vl-fp8-service/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1692: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m   torch._dynamo.utils.warn_once(msg)
[1;36m(Worker_TP4_EP4 pid=3057465)[0;0m /home/naresh/venvs/qwen3-vl-fp8-service/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1692: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1;36m(Worker_TP4_EP4 pid=3057465)[0;0m   torch._dynamo.utils.warn_once(msg)
[1;36m(Worker_TP2_EP2 pid=3057463)[0;0m /home/naresh/venvs/qwen3-vl-fp8-service/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1692: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1;36m(Worker_TP2_EP2 pid=3057463)[0;0m   torch._dynamo.utils.warn_once(msg)
[1;36m(Worker_TP3_EP3 pid=3057464)[0;0m /home/naresh/venvs/qwen3-vl-fp8-service/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1692: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
[1;36m(Worker_TP3_EP3 pid=3057464)[0;0m   torch._dynamo.utils.warn_once(msg)
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m INFO 12-02 09:56:55 [backends.py:631] Using cache directory: /home/naresh/.cache/vllm/torch_compile_cache/f144b4c44b/rank_0_0/backbone for vLLM's torch.compile
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m INFO 12-02 09:56:55 [backends.py:647] Dynamo bytecode transform time: 22.08 s
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m INFO 12-02 09:57:05 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.982 s
[1;36m(Worker_TP5_EP5 pid=3057466)[0;0m INFO 12-02 09:57:06 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.835 s
[1;36m(Worker_TP1_EP1 pid=3057462)[0;0m INFO 12-02 09:57:06 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.066 s
[1;36m(Worker_TP7_EP7 pid=3057468)[0;0m INFO 12-02 09:57:07 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.435 s
[1;36m(Worker_TP3_EP3 pid=3057464)[0;0m INFO 12-02 09:57:08 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 11.276 s
[1;36m(Worker_TP2_EP2 pid=3057463)[0;0m INFO 12-02 09:57:13 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.209 s
[1;36m(Worker_TP6_EP6 pid=3057467)[0;0m INFO 12-02 09:57:16 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 10.760 s
[1;36m(Worker_TP4_EP4 pid=3057465)[0;0m INFO 12-02 09:57:17 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.990 s
[1;36m(Worker_TP1_EP1 pid=3057462)[0;0m WARNING 12-02 09:57:22 [fused_moe.py:886] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/naresh/venvs/qwen3-vl-fp8-service/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=16,N=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json']
[1;36m(Worker_TP5_EP5 pid=3057466)[0;0m WARNING 12-02 09:57:22 [fused_moe.py:886] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/naresh/venvs/qwen3-vl-fp8-service/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=16,N=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json']
[1;36m(Worker_TP2_EP2 pid=3057463)[0;0m WARNING 12-02 09:57:22 [fused_moe.py:886] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/naresh/venvs/qwen3-vl-fp8-service/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=16,N=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json']
[1;36m(Worker_TP4_EP4 pid=3057465)[0;0m WARNING 12-02 09:57:22 [fused_moe.py:886] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/naresh/venvs/qwen3-vl-fp8-service/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=16,N=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json']
[1;36m(Worker_TP3_EP3 pid=3057464)[0;0m WARNING 12-02 09:57:22 [fused_moe.py:886] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/naresh/venvs/qwen3-vl-fp8-service/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=16,N=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json']
[1;36m(Worker_TP7_EP7 pid=3057468)[0;0m WARNING 12-02 09:57:22 [fused_moe.py:886] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/naresh/venvs/qwen3-vl-fp8-service/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=16,N=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json']
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m WARNING 12-02 09:57:22 [fused_moe.py:886] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/naresh/venvs/qwen3-vl-fp8-service/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=16,N=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json']
[1;36m(Worker_TP6_EP6 pid=3057467)[0;0m WARNING 12-02 09:57:22 [fused_moe.py:886] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/naresh/venvs/qwen3-vl-fp8-service/lib/python3.10/site-packages/vllm/model_executor/layers/fused_moe/configs/E=16,N=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json']
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m INFO 12-02 09:57:22 [monitor.py:34] torch.compile takes 31.06 s in total
[1;36m(EngineCore_DP0 pid=3057393)[0;0m INFO 12-02 09:57:23 [shm_broadcast.py:501] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m INFO 12-02 09:57:24 [gpu_worker.py:359] Available KV cache memory: 35.80 GiB
[1;36m(EngineCore_DP0 pid=3057393)[0;0m INFO 12-02 09:57:25 [kv_cache_utils.py:1229] GPU KV cache size: 798,736 tokens
[1;36m(EngineCore_DP0 pid=3057393)[0;0m INFO 12-02 09:57:25 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 6.09x
[1;36m(EngineCore_DP0 pid=3057393)[0;0m INFO 12-02 09:57:25 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 6.09x
[1;36m(EngineCore_DP0 pid=3057393)[0;0m INFO 12-02 09:57:25 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 6.09x
[1;36m(EngineCore_DP0 pid=3057393)[0;0m INFO 12-02 09:57:25 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 6.09x
[1;36m(EngineCore_DP0 pid=3057393)[0;0m INFO 12-02 09:57:25 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 6.09x
[1;36m(EngineCore_DP0 pid=3057393)[0;0m INFO 12-02 09:57:25 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 6.09x
[1;36m(EngineCore_DP0 pid=3057393)[0;0m INFO 12-02 09:57:25 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 6.09x
[1;36m(EngineCore_DP0 pid=3057393)[0;0m INFO 12-02 09:57:25 [kv_cache_utils.py:1234] Maximum concurrency for 131,072 tokens per request: 6.09x
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m 2025-12-02 09:57:25,805 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[1;36m(Worker_TP2_EP2 pid=3057463)[0;0m 2025-12-02 09:57:25,805 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[1;36m(Worker_TP5_EP5 pid=3057466)[0;0m 2025-12-02 09:57:25,805 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[1;36m(Worker_TP1_EP1 pid=3057462)[0;0m 2025-12-02 09:57:25,805 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[1;36m(Worker_TP6_EP6 pid=3057467)[0;0m 2025-12-02 09:57:25,805 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[1;36m(Worker_TP3_EP3 pid=3057464)[0;0m 2025-12-02 09:57:25,805 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[1;36m(Worker_TP4_EP4 pid=3057465)[0;0m 2025-12-02 09:57:25,805 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[1;36m(Worker_TP7_EP7 pid=3057468)[0;0m 2025-12-02 09:57:25,805 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[1;36m(Worker_TP2_EP2 pid=3057463)[0;0m 2025-12-02 09:57:26,136 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[1;36m(Worker_TP6_EP6 pid=3057467)[0;0m 2025-12-02 09:57:26,136 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m 2025-12-02 09:57:26,136 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[1;36m(Worker_TP1_EP1 pid=3057462)[0;0m 2025-12-02 09:57:26,136 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[1;36m(Worker_TP5_EP5 pid=3057466)[0;0m 2025-12-02 09:57:26,136 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[1;36m(Worker_TP3_EP3 pid=3057464)[0;0m 2025-12-02 09:57:26,136 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[1;36m(Worker_TP4_EP4 pid=3057465)[0;0m 2025-12-02 09:57:26,136 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[1;36m(Worker_TP7_EP7 pid=3057468)[0;0m 2025-12-02 09:57:26,136 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  3.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  3.23it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.61it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.55it/s]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s][1;36m(Worker_TP6_EP6 pid=3057467)[0;0m INFO 12-02 09:57:27 [custom_all_reduce.py:216] Registering 940 cuda graph addresses
[1;36m(Worker_TP1_EP1 pid=3057462)[0;0m INFO 12-02 09:57:27 [custom_all_reduce.py:216] Registering 940 cuda graph addresses
Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.90it/s][1;36m(Worker_TP5_EP5 pid=3057466)[0;0m INFO 12-02 09:57:27 [custom_all_reduce.py:216] Registering 940 cuda graph addresses
[1;36m(Worker_TP3_EP3 pid=3057464)[0;0m INFO 12-02 09:57:27 [custom_all_reduce.py:216] Registering 940 cuda graph addresses
[1;36m(Worker_TP2_EP2 pid=3057463)[0;0m INFO 12-02 09:57:27 [custom_all_reduce.py:216] Registering 940 cuda graph addresses
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.10it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.06it/s]
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m INFO 12-02 09:57:27 [custom_all_reduce.py:216] Registering 940 cuda graph addresses
[1;36m(Worker_TP7_EP7 pid=3057468)[0;0m INFO 12-02 09:57:28 [custom_all_reduce.py:216] Registering 940 cuda graph addresses
[1;36m(Worker_TP4_EP4 pid=3057465)[0;0m INFO 12-02 09:57:28 [custom_all_reduce.py:216] Registering 940 cuda graph addresses
[1;36m(Worker_TP0_EP0 pid=3057461)[0;0m INFO 12-02 09:57:29 [gpu_model_runner.py:4244] Graph capturing finished in 3 secs, took -0.88 GiB
[1;36m(EngineCore_DP0 pid=3057393)[0;0m INFO 12-02 09:57:29 [core.py:250] init engine (profile, create kv cache, warmup model) took 66.04 seconds
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [api_server.py:1725] Supported tasks: ['generate']
[1;36m(APIServer pid=3057302)[0;0m WARNING 12-02 09:57:37 [model.py:1568] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [serving_responses.py:154] Using default chat sampling params from model: {'temperature': 0.8, 'top_k': 20, 'top_p': 0.95}
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [serving_chat.py:131] Using default chat sampling params from model: {'temperature': 0.8, 'top_k': 20, 'top_p': 0.95}
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [serving_completion.py:73] Using default completion sampling params from model: {'temperature': 0.8, 'top_k': 20, 'top_p': 0.95}
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [serving_chat.py:131] Using default chat sampling params from model: {'temperature': 0.8, 'top_k': 20, 'top_p': 0.95}
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [api_server.py:2052] Starting vLLM API server 0 on http://0.0.0.0:8010
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:38] Available routes are:
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /health, Methods: GET
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /load, Methods: GET
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /version, Methods: GET
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /v1/messages, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /pooling, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /classify, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /score, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /rerank, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /ping, Methods: GET
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /ping, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /invocations, Methods: POST
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:57:37 [launcher.py:46] Route: /metrics, Methods: GET
[1;36m(APIServer pid=3057302)[0;0m INFO:     Started server process [3057302]
[1;36m(APIServer pid=3057302)[0;0m INFO:     Waiting for application startup.
[1;36m(APIServer pid=3057302)[0;0m INFO:     Application startup complete.
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:58:54 [chat_utils.py:557] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:59:28 [loggers.py:236] Engine 000: Avg prompt throughput: 890.7 tokens/s, Avg generation throughput: 36.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:59:38 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:59:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 09:59:58 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:00:08 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:00:18 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:00:28 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:00:38 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:00:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:00:58 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:01:08 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:01:18 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO:     172.16.5.15:57802 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:01:28 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 34.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:01:38 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:31:18 [loggers.py:236] Engine 000: Avg prompt throughput: 820.5 tokens/s, Avg generation throughput: 34.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 1.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:31:28 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 1.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:31:38 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 1.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:31:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 1.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:31:58 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 1.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:32:08 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 1.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:32:18 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 1.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:32:28 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 1.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:32:38 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 1.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO:     172.16.5.15:33554 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:32:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 39.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:32:58 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:39:28 [loggers.py:236] Engine 000: Avg prompt throughput: 927.3 tokens/s, Avg generation throughput: 39.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 1.7%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:39:38 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 1.7%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:39:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 1.7%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:39:58 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 1.7%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:40:08 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 1.7%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:40:18 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 1.7%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:40:28 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 1.7%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:40:38 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 1.7%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:40:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 1.7%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:40:58 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 1.7%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:41:08 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 1.7%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:41:18 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 1.7%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO:     172.16.9.151:34526 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:41:28 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 29.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.7%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:41:38 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.7%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:42:38 [loggers.py:236] Engine 000: Avg prompt throughput: 722.1 tokens/s, Avg generation throughput: 27.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:42:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:42:58 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:43:08 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:43:18 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:43:28 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:43:38 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:43:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:43:58 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:44:08 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:44:18 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:44:28 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:44:38 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:44:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:44:58 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:45:08 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO:     172.16.9.151:48896 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:45:18 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:45:28 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 2.0%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:52:28 [loggers.py:236] Engine 000: Avg prompt throughput: 636.3 tokens/s, Avg generation throughput: 16.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:52:38 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:52:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:52:58 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:53:08 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:53:18 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:53:28 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:53:38 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:53:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:53:58 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:54:08 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:54:18 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:54:28 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:54:38 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:54:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:54:58 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:55:08 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:55:18 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.1%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:55:28 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:55:38 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:55:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:55:58 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:56:08 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:56:18 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.5%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:56:28 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.6%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO:     172.16.9.151:36506 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:56:38 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 10:56:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 2.2%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 11:13:38 [loggers.py:236] Engine 000: Avg prompt throughput: 718.8 tokens/s, Avg generation throughput: 45.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 2.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 11:13:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 2.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 11:13:58 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 2.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 11:14:08 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 2.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 11:14:18 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 2.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 11:14:28 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 2.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 11:14:38 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 2.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 11:14:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 2.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 11:14:58 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 2.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 11:15:08 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 2.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 11:15:18 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 2.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 11:15:28 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 2.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 11:15:38 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 2.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 11:15:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 58.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 2.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO:     172.16.5.15:53314 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 11:15:58 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 49.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 2.4%, MM cache hit rate: 0.0%
[1;36m(APIServer pid=3057302)[0;0m INFO 12-02 11:16:08 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 2.4%, MM cache hit rate: 0.0%
